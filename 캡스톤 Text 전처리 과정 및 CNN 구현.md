# 캡스톤 Text 전처리 과정 및 CNN 구현

1. csv 파일에서 10개 미만의 데이터를 가진 label은 학습에서 제외시키도록 하겠습니다.
2. 토큰화
- KoNLPy를 이용하여 토큰화를 진행해보겠습니다.
- 다양한 방법들이 있지만 최근에 성능이 좋다고 알려진 Mecab을 먼저 사용해보겠습니다.
- 전체적인 코드가 잘 작동되어 학습이 이루어지면 그 이후에 Okt, Kkma, Komoran 등의 클래스를 사용해보아 성능을 비교해보겠습니다.
3. 불용어
- 불용어 제거 과정없이 토큰화 단계에서 명사를 추출해볼 계획입니다.
- 명사만 추출하게 될 경우 label을 맞추는데 있어서 덜 중요한 단어들이 학습되는 것을 방지할 수 있다고 생각했습니다.
4. Text에 대한 정수 인코딩
- 각 단어를 고유한 숫자에 맵핑시키는 과정입니다.
- Word2Vec을 이용해볼 계획입니다. 주어진 데이터에 대해 가중치 행렬을 학습하는데, 여기서 사용자인 제가 지정해야 하는 값은 가중치 행렬의 차원입니다.
- 원핫 인코딩에서 단어를 표현하기 위해 전체 단어의 개수만큼 열을 생성하는 것과 달리 Word2Vec은 현저히 작은 차원으로 단어표현이 가능한 장점이 있습니다.
- Word2Vec 이외에 Doc2Vec, FastText, LDA Model 등과 같은 자연어 처리에 사용도는 모델들이 있는데 먼저 Word2Vec을 사용하여 학습을 한 번 진행한뒤 다른 모델들을 사용하여 성능을 비교해보겠습니다.
5. label에 대한 정수 인코딩
- 원.핫.인코딩 방식을 사용하겠습니다.
- Word2Vec와 같은 모델들은 유사도를 고려하는데, label은 label 간 유사도보다는 각각의 label이 별개로 학습되어야 하기 때문입니다.
6. 데이터의 분리
- 데이터를 Train, Validation set으로 랜덤하게 분할하여 저장하겠습니다.
- numpy 패키지의 RandomState를 사용하겠습니다.
7. Field 클래스를 정의합니다.
- Text에 해당하는 OVERVIEW와 label에 해당하는 CAT3로 나누어 필드를 생성하려고 합니다.
8. (OVERVIEW, LABEL) 형식을 가진 데이터셋을 생성
- torchtext에서 제공하는 TabularDataset을 사용할 예정입니다. csv 파일을 읽어 (OVERVIEW, LABEL) 형식을 가진 데이터셋을 생성하는데 적합하다고 생각했습니다.
9. 데이터 로드
- OVERVIEW와 CAT3의 단어장을 생성합니다.
- Word2Vec를 통해 훈련시켰던 임베딩 벡터를 단어장에 세팅합니다.
- BucketIterator를 통해 데이터를 로드시켜보겠습니다. 앞서 TabularDataset을 통해 생성한 데이터셋을 로드시키려고 합니다. 로드할 배치의 사이즈를 조정해야 합니다.
10. 학습 모델 설계
- CNN
- RNN
- 제가 고려해본 학습 모델은 CNN과 RNN입니다.
- RNN은 오랜기간 텍스트 학습에 사용되어 왔고, CNN은 2014년 이후부터 텍스트 학습에 사용되기 시작했습니다.
- RNN의 성능이 더 우월하게 나올 수는 있으나, 먼저 CNN을 활용하여 첫 학습을 해본 뒤, RNN을 사용하여 성능을 비교해보겠습니다.
- 앞선 데이터 전처리 작업을 진행하고 Input하게 되면 입력되는 데이터는 행은 쓰인 단어의 개수이며, 열은 임베딩을 할 때 제가 지정한 차원의 수가 됩니다. 예를 들면, 50개의 단어가 쓰였고, 제가 차원을 100으로 지정했다면, 50x100 사이즈의 행렬이 입력됩니다. 여기서, Word2Vec과 같은 모델이 원핫인코딩과 비교되는 점은 원핫인코딩한 경우 총 단어개수가 45000개라면, 50x45000 사이즈의 행렬이 입력되는데, Word2Vec과 같은 모델을 사용하면 제가 지정한 차원의 개수만큼만 열을 생성하게 된다는 장점이 있습니다. 또한, 45000개에서 100개의 차원으로 줄었지만 Word2Vec은 단어간 유사도 또한 학습하기 때문에 적절한 차원으로 조정할 경우 원핫인코딩보다 더 나은 학습 결과가 나올 것이라고 예상됩니다. 우선, Word2Vec을 사용한 모델을 완성한 뒤 Word2Vec이 아닌 원핫인코딩을 사용하여 성능비교를 해보겠습니다.
- CNN을 설계할 때, Filter의 개수, Filter의 사이즈, 패딩여부, 활성화 함수 등을 고려하여 선정해야 합니다.
- 풀링 층 또한 고려해야 하는데, 최대값, 중간값, 최소값을 선정하는 방식이 있습니다. 먼저 최대값을 선정하는 Max-pooling 방식을 사용하겠습니다.
- 또, CNN은 1d와 2d 방식이 있는데, text 분야에서 많이 사용되는 1d convolutions 방식을 먼저 사용하겠습니다.
- CNN 구현 방식은 케라스, 파이토치 두 가지를 사용해보려고 합니다. 먼저, 케라스로 구현해보겠습니다.